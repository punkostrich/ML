#################################
### Homework 4.1.2a
#################################
import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt
import scipy.io as spio
import scipy.sparse.linalg as ll
import sklearn.preprocessing as skpp
import matplotlib.image as img
from scipy import misc
import scipy as scipy
import scipy.io as sio
from scipy.sparse import csc_matrix, find
import matplotlib
import os
import time
from PIL import Image
import glob
import cv2
import skimage
from skimage.transform import resize
from sklearn.decomposition import PCA
from scipy import stats
from sklearn.neighbors import KernelDensity
from matplotlib.pyplot import plot
from scipy.io import loadmat
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn import metrics
from sklearn.datasets import make_moons, make_circles, make_classification
from sklearn.preprocessing import StandardScaler
from matplotlib.colors import ListedColormap
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report,confusion_matrix

# Main
# loading the gif image, downscale by a factor of 4
cwd = os.getcwd()
X_train = scipy.io.loadmat(cwd+'\\data\\mnist_10digits.mat')['xtrain']/255.
y_train = scipy.io.loadmat(cwd+'\\data\\mnist_10digits.mat')['ytrain'].ravel()
X_test = scipy.io.loadmat(cwd+'\\data\\mnist_10digits.mat')['xtest']/255.
y_test = scipy.io.loadmat(cwd+'\\data\\mnist_10digits.mat')['ytest'].ravel()

#=====================================
#  Tuning KNN for K
#=====================================
#n_samples = len(X_train)
#params = {
#        'n_neighbors': [2, 3, 5, 7, 9, 11] 
#    }
#clf = GridSearchCV(estimator=KNeighborsClassifier(),           
#                      param_grid=params, 
#                      cv=5,
#                      return_train_score=True) 
#model = clf.fit(X_train[:n_samples // 10],y_train[:n_samples // 10])
#Print The value of best Hyperparameters
#print('Best n_neighbors (KNN):', model.best_estimator_.get_params()['n_neighbors'])

#=====================================
#  Tuning SVC for gamma
#=====================================
#params = {'gamma': [1e-2, 1e-3, 1e-4],
#          'C': [1e-1, 1, 10]}

#clf = GridSearchCV(estimator=SVC(),           
#                      param_grid=params, 
#                      cv=3,
#                      return_train_score=True) 
#model = clf.fit(X_train[:n_samples // 10],y_train[:n_samples // 10])
#Print The value of best Hyperparameters
#print('Best params (SVC):', model.best_estimator_.get_params())


######################################################
##      Performance of different classifiers        ##
######################################################
def confusion(y_test, y_pred):
    conf = pd.DataFrame(confusion_matrix(y_test, y_pred),
                        index=['True[0]', 'True[1]','True[2]','True[3]','True[4]','True[5]','True[6]', 'True[7]', 'True[8]', 'True[9]'],
                        columns=['Predict[0]', 'Predict[1]', 'Predict[2]', 'Predict[3]', 'Predict[4]', 'Predict[5]', 'Predict[6]', 'Predict[7]', 'Predict[8]', 'Predict[9]'])
    print('Confusion Matrix:')
    print(conf)
    return conf


names = ["KNN", "Logistic Regression", "SVM", "kernel SVM", "Neural Networks"]

classifiers = [
    KNeighborsClassifier(3),
    LogisticRegression(max_iter=1e5),
    SVC(kernel='linear'),
    SVC(gamma=0.01, C = 10, random_state = 1),
    MLPClassifier(hidden_layer_sizes=(20,10), max_iter=200, alpha=1e-4,
                    solver='sgd', verbose=10, random_state=1,
                    learning_rate_init=.1)
    ]

pd.set_option('display.max_columns', None)
# iterate over classifiers
for name, clf in zip(names, classifiers):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)

    print('~~~~~~~~~~~~~~~~~~~~~~~~~~  '+ str(name) +' ~~~~~~~~~~~~~~~~~~~~~~~~~~~')
    confusion(y_test, y_pred)
    print('')
    print('~~~~~~~~~~~~~~~~~~~~~~~~~~  '+ str(name) + " Score " +' ~~~~~~~~~~~~~~~~~~~~~~~~~~~')
    print(classification_report(y_test,y_pred))



   
        



